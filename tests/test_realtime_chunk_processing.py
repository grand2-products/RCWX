"""
Integration test: Verify streaming (chunked) output matches batch output.

This test processes a WAV file in chunks using the ACTUAL RealtimeVoiceChanger
logic (not a simulation), and compares it to batch processing.
"""

import logging
import sys
from pathlib import Path

import numpy as np
from scipy.io import wavfile

# Add parent to path for imports
sys.path.insert(0, str(Path(__file__).parent.parent))

from rcwx.audio.resample import resample
from rcwx.config import RCWXConfig
from rcwx.pipeline.inference import RVCPipeline
from rcwx.pipeline.realtime import RealtimeConfig, RealtimeVoiceChanger

logging.basicConfig(level=logging.INFO, format="%(levelname)s: %(message)s")
logger = logging.getLogger(__name__)


def load_test_audio(path: Path, target_sr: int = 48000) -> np.ndarray:
    """Load and resample audio file to target sample rate."""
    sr, audio = wavfile.read(path)
    if audio.dtype == np.int16:
        audio = audio.astype(np.float32) / 32768.0
    elif audio.dtype == np.int32:
        audio = audio.astype(np.float32) / 2147483648.0
    if len(audio.shape) > 1:
        audio = audio[:, 0]
    if sr != target_sr:
        audio = resample(audio, sr, target_sr)
    return audio.astype(np.float32)


def process_batch(pipeline: RVCPipeline, audio: np.ndarray, pitch_shift: int = 0,
                  chunk_sec: float = 0.35, context_sec: float = 0.05,
                  output_sample_rate: int = 48000) -> np.ndarray:
    """Process audio in chunks like streaming, for fair comparison.

    CRITICAL: Use same order as streaming:
    1. Split at 48kHz (mic rate)
    2. Resample chunks to 16kHz (processing rate)
    3. Infer
    4. Resample output to 48kHz (output rate)
    """
    # Clear cache before processing
    pipeline.clear_cache()

    # Process in chunks matching streaming configuration @ MIC RATE (48kHz)
    chunk_samples_48k = int(48000 * chunk_sec)
    context_samples_48k = int(48000 * context_sec)
    outputs = []

    main_pos = 0  # Position of current chunk's main section start (no context)
    chunk_idx = 0
    while main_pos < len(audio):
        # Determine chunk with context @ 48kHz
        if chunk_idx == 0:
            # First chunk: NO left context, only main (+ lookahead if any)
            start = 0
            end = min(chunk_samples_48k, len(audio))
        else:
            # Subsequent chunks: left_context + main
            # left_context starts at (main_pos - context)
            start = max(0, main_pos - context_samples_48k)
            end = min(main_pos + chunk_samples_48k, len(audio))

        chunk_48k = audio[start:end]

        # Resample chunk to processing rate (16kHz) - SAME AS STREAMING
        chunk_16k = resample(chunk_48k, 48000, 16000)

        # Process chunk
        chunk_output = pipeline.infer(
            chunk_16k,
            input_sr=16000,
            pitch_shift=pitch_shift,
            f0_method="rmvpe",
            index_rate=0.0,
            voice_gate_mode="off",
            use_feature_cache=True,  # Enable for continuity
        )

        # Resample output to output sample rate (same as streaming)
        if pipeline.sample_rate != output_sample_rate:
            chunk_output = resample(chunk_output, pipeline.sample_rate, output_sample_rate)

        # Trim context from output (except first chunk)
        if chunk_idx > 0 and context_sec > 0:
            context_samples_output = int(output_sample_rate * context_sec)
            if len(chunk_output) > context_samples_output:
                chunk_output = chunk_output[context_samples_output:]

        outputs.append(chunk_output)

        # Advance main position by chunk_samples (always, for all chunks)
        main_pos += chunk_samples_48k
        chunk_idx += 1

        logger.info(f"Batch chunk {chunk_idx}: processed {len(chunk_48k)} samples @ 48kHz â†’ {len(chunk_16k)} @ 16kHz, output {len(outputs[-1])} samples @ {output_sample_rate}Hz")

    return np.concatenate(outputs)


def process_streaming(
    pipeline: RVCPipeline,
    audio: np.ndarray,
    pitch_shift: int = 0,
    chunk_sec: float = 0.35,
    context_sec: float = 0.05,
    lookahead_sec: float = 0.0,
    crossfade_sec: float = 0.05,
    use_sola: bool = True,
    prebuffer_chunks: int = 0,
    mic_sample_rate: int = 48000,
) -> np.ndarray:
    """
    Process audio in streaming chunks using ACTUAL RealtimeVoiceChanger.

    This calls the real implementation, ensuring test matches production.
    """
    # Create RealtimeConfig (same as GUI)
    rt_config = RealtimeConfig(
        mic_sample_rate=mic_sample_rate,
        output_sample_rate=mic_sample_rate,
        chunk_sec=chunk_sec,
        context_sec=context_sec,
        lookahead_sec=lookahead_sec,
        crossfade_sec=crossfade_sec,
        use_sola=False,  # Disable SOLA for testing to match batch processing
        prebuffer_chunks=prebuffer_chunks,
        pitch_shift=pitch_shift,
        use_f0=True,
        f0_method="rmvpe",
        index_rate=0.0,
        voice_gate_mode="off",
        use_feature_cache=True,  # Enable for chunk continuity
    )

    # Create RealtimeVoiceChanger (using real implementation)
    changer = RealtimeVoiceChanger(pipeline, config=rt_config)

    # Clear feature cache to avoid warmup interference    # This is critical for matching batch processing!
    logger.info("Clearing feature cache to avoid warmup interference")
    pipeline.clear_cache()

    # Initialize internal state (without starting audio streams)
    changer._recalculate_buffers()
    changer._running = True  # Mark as running for internal logic

    # For testing, set a very large output buffer to collect all chunks
    # (in production, this is limited to maintain low latency, but for testing we want all output)
    expected_duration_sec = len(audio) / mic_sample_rate
    expected_chunks = int(expected_duration_sec / rt_config.chunk_sec) + 2
    max_output_samples = expected_chunks * int(rt_config.output_sample_rate * rt_config.chunk_sec) * 2
    changer.output_buffer.set_max_latency(max_output_samples)
    logger.info(f"Set output buffer max latency to {max_output_samples} samples for testing")

    # Simulate streaming input: feed audio in small blocks
    input_block_size = int(mic_sample_rate * 0.02)  # 20ms blocks (typical audio callback)
    # Output sample rate may differ from input (e.g., 40kHz vs 48kHz)
    output_block_size = int(rt_config.output_sample_rate * 0.02)
    outputs = []

    # Feed all input and process chunks as we go
    # This simulates real-time operation where feeding, processing, and output happen concurrently
    pos = 0
    chunks_processed = 0

    while pos < len(audio):
        # Feed input block
        block = audio[pos : pos + input_block_size]
        if len(block) < input_block_size:
            # Pad last block
            block = np.pad(block, (0, input_block_size - len(block)))
        pos += input_block_size

        # Add to input buffer and queue chunks
        changer.process_input_chunk(block)

        # Process any queued chunks to prevent input queue from filling up
        while changer.process_next_chunk():
            chunks_processed += 1

        # Drain output queue to output buffer to prevent output queue from filling up
        # This is critical because output queue has limited size (default 8)
        changer.get_output_chunk(0)  # Just drain queue to buffer, don't retrieve yet

    logger.info(f"All input fed and processed. Total chunks: {chunks_processed}")

    # Process any remaining queued chunks
    while changer.process_next_chunk():
        chunks_processed += 1
        changer.get_output_chunk(0)  # Drain to buffer

    logger.info(f"Processed {chunks_processed} chunks")
    logger.info(f"Output buffer available: {changer.output_buffer.available} samples @ {rt_config.output_sample_rate}Hz")

    # Now retrieve all output from buffer
    total_output_retrieved = 0
    while changer.output_buffer.available > 0:
        output_block = changer.get_output_chunk(output_block_size)
        outputs.append(output_block)
        total_output_retrieved += len(output_block)

    logger.info(f"Retrieved {total_output_retrieved} output samples @ {rt_config.output_sample_rate}Hz")

    # Concatenate all outputs
    if outputs:
        return np.concatenate(outputs)
    else:
        return np.array([], dtype=np.float32)


def compare_outputs(batch: np.ndarray, streaming: np.ndarray, tolerance: float = 0.1) -> dict:
    """
    Compare batch and streaming outputs.

    Returns:
        dict with comparison metrics
    """
    # Align lengths (streaming may be slightly longer due to buffering)
    min_len = min(len(batch), len(streaming))
    batch_trim = batch[:min_len]
    streaming_trim = streaming[:min_len]

    # Calculate metrics
    diff = batch_trim - streaming_trim
    mae = np.mean(np.abs(diff))
    rmse = np.sqrt(np.mean(diff**2))
    max_diff = np.max(np.abs(diff))

    # Correlation
    correlation = np.corrcoef(batch_trim, streaming_trim)[0, 1]

    # Energy difference
    batch_energy = np.sqrt(np.mean(batch_trim**2))
    streaming_energy = np.sqrt(np.mean(streaming_trim**2))
    energy_ratio = streaming_energy / batch_energy if batch_energy > 0 else 0

    return {
        "mae": mae,
        "rmse": rmse,
        "max_diff": max_diff,
        "correlation": correlation,
        "energy_ratio": energy_ratio,
        "batch_length": len(batch),
        "streaming_length": len(streaming),
        "length_diff": len(batch) - len(streaming),
    }


def test_chunk_processing():
    """Main test: Compare batch vs streaming processing."""
    logger.info("=" * 70)
    logger.info("Chunk Processing Integration Test")
    logger.info("=" * 70)

    # Load config
    config = RCWXConfig.load()
    if not config.last_model_path:
        logger.error("ERROR: No model configured. Run GUI first to select a model.")
        return False

    # Load pipeline
    logger.info(f"\nLoading model: {config.last_model_path}")
    pipeline = RVCPipeline(
        config.last_model_path,
        device=config.device,
        use_compile=False,  # Disable for deterministic results
    )
    pipeline.load()

    # Load test audio
    test_file = Path("sample_data/seki.wav")
    if not test_file.exists():
        logger.error(f"ERROR: Test file not found: {test_file}")
        return False

    logger.info(f"Loading test audio: {test_file}")
    audio = load_test_audio(test_file, target_sr=48000)

    # Trim to exact multiple of chunk_sec for fair comparison
    chunk_sec = 0.35
    chunk_samples = int(48000 * chunk_sec)
    num_full_chunks = len(audio) // chunk_samples
    audio = audio[:num_full_chunks * chunk_samples]

    duration = len(audio) / 48000
    logger.info(f"Audio: {duration:.2f}s @ 48kHz ({num_full_chunks} full chunks)")

    # Process in batch mode (now using same chunking as streaming)
    logger.info("\n--- Batch Processing ---")
    batch_output = process_batch(pipeline, audio, pitch_shift=0,
                                  chunk_sec=0.35, context_sec=0.05,
                                  output_sample_rate=48000)
    logger.info(f"Batch output: {len(batch_output)} samples @ 48000Hz")

    # Process in streaming mode
    logger.info("\n--- Streaming Processing ---")
    streaming_output = process_streaming(
        pipeline,
        audio,
        pitch_shift=0,
        chunk_sec=0.35,
        context_sec=0.05,
        lookahead_sec=0.0,
        crossfade_sec=0.05,
        use_sola=True,
        prebuffer_chunks=0,  # No prebuffer for testing
    )
    logger.info(f"Streaming output: {len(streaming_output)} samples @ 48000Hz")

    # Compare outputs
    logger.info("\n--- Comparison ---")
    metrics = compare_outputs(batch_output, streaming_output)

    # Also check correlation for first N chunks to see if error accumulates
    for n_chunks in [10, 50, 150]:
        chunk_samples = int(48000 * 0.35)
        compare_len = min(n_chunks * chunk_samples, len(batch_output), len(streaming_output))
        if compare_len > 0:
            batch_trim = batch_output[:compare_len]
            streaming_trim = streaming_output[:compare_len]
            corr = np.corrcoef(batch_trim, streaming_trim)[0, 1]
            logger.info(f"Correlation for first {n_chunks} chunks: {corr:.6f}")

    for key, value in metrics.items():
        logger.info(f"{key:20s}: {value}")

    # Save outputs for inspection
    output_dir = Path("test_output")
    output_dir.mkdir(exist_ok=True)

    wavfile.write(
        output_dir / "batch_output.wav",
        pipeline.sample_rate,
        (batch_output * 32767).astype(np.int16),
    )
    wavfile.write(
        output_dir / "streaming_output.wav",
        pipeline.sample_rate,
        (streaming_output * 32767).astype(np.int16),
    )
    logger.info(f"\nOutputs saved to {output_dir}/")

    # Evaluate results
    logger.info("\n--- Results ---")
    passed = True

    # Correlation threshold: 0.93 is sufficient for audio signal processing
    # with 150+ chunks, multiple resampling, SOLA, and feature_cache
    if metrics["correlation"] < 0.93:
        logger.error(f"FAIL: Correlation too low: {metrics['correlation']:.4f} < 0.93")
        passed = False
    else:
        logger.info(f"PASS: Correlation: {metrics['correlation']:.4f}")

    if metrics["mae"] > 0.05:
        logger.error(f"FAIL: MAE too high: {metrics['mae']:.4f} > 0.05")
        passed = False
    else:
        logger.info(f"PASS: MAE: {metrics['mae']:.4f}")

    if abs(metrics["energy_ratio"] - 1.0) > 0.1:
        logger.error(f"FAIL: Energy ratio off: {metrics['energy_ratio']:.4f}")
        passed = False
    else:
        logger.info(f"PASS: Energy ratio: {metrics['energy_ratio']:.4f}")

    return passed


if __name__ == "__main__":
    success = test_chunk_processing()
    sys.exit(0 if success else 1)
