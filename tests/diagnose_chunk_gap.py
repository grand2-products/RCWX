"""
éŸ³å£°é€”åˆ‡ã‚Œè¨ºæ–­ã‚¹ã‚¯ãƒªãƒ—ãƒˆ

å®Ÿéš›ã®RealtimeVoiceChangerã‚’ä½¿ç”¨ã—ã¦ã€ãƒãƒ£ãƒ³ã‚¯å¢ƒç•Œã§ã®
éŸ³å£°ã®é€”åˆ‡ã‚Œã‚’å¯è¦–åŒ–ãƒ»æ¸¬å®šã—ã¾ã™ã€‚
"""

import time
from pathlib import Path

import matplotlib.pyplot as plt
import numpy as np
from scipy.io import wavfile

from rcwx.config import RCWXConfig
from rcwx.pipeline.inference import RVCPipeline
from rcwx.pipeline.realtime import RealtimeConfig, RealtimeVoiceChanger


def generate_test_signal(duration_sec: float = 5.0, sr: int = 48000) -> np.ndarray:
    """ãƒ†ã‚¹ãƒˆä¿¡å·ç”Ÿæˆ: å®‰å®šã—ãŸ220Hz + æŒ¯å¹…å¤‰èª¿"""
    t = np.arange(int(sr * duration_sec)) / sr
    # åŸºæœ¬æ³¢ 220Hz
    fundamental = np.sin(2 * np.pi * 220 * t)
    # ã‚†ã£ãã‚Šã—ãŸæŒ¯å¹…å¤‰èª¿ (5Hz)
    modulation = 0.5 + 0.5 * np.sin(2 * np.pi * 5 * t)
    return (fundamental * modulation * 0.5).astype(np.float32)


def detect_gaps(audio: np.ndarray, sr: int, threshold_db: float = -40) -> list[dict]:
    """éŸ³å£°ã®é€”åˆ‡ã‚Œï¼ˆgapï¼‰ã‚’æ¤œå‡º"""
    # çŸ­æ™‚é–“ã‚¨ãƒãƒ«ã‚®ãƒ¼ (10ms window)
    window = int(sr * 0.01)
    energies = []
    positions = []

    for i in range(0, len(audio) - window, window // 2):
        energy = np.sqrt(np.mean(audio[i:i + window] ** 2))
        energy_db = 20 * np.log10(energy + 1e-10)
        energies.append(energy_db)
        positions.append(i / sr)  # ç§’å˜ä½

    energies = np.array(energies)
    positions = np.array(positions)

    # é€”åˆ‡ã‚Œã®æ¤œå‡º: ã‚¨ãƒãƒ«ã‚®ãƒ¼ãŒé–¾å€¤ä»¥ä¸‹
    gaps = []
    in_gap = False
    gap_start = 0

    for i, (pos, energy) in enumerate(zip(positions, energies)):
        if energy < threshold_db:
            if not in_gap:
                gap_start = pos
                in_gap = True
        else:
            if in_gap:
                gaps.append({
                    'start': gap_start,
                    'end': pos,
                    'duration': pos - gap_start,
                    'min_energy': energies[i-1] if i > 0 else energy,
                })
                in_gap = False

    return gaps


def analyze_chunk_boundaries(
    audio: np.ndarray,
    sr: int,
    chunk_sec: float,
    boundary_window_ms: float = 50,
) -> list[dict]:
    """ãƒãƒ£ãƒ³ã‚¯å¢ƒç•Œä»˜è¿‘ã®ã‚¨ãƒãƒ«ã‚®ãƒ¼ã‚’åˆ†æ"""
    chunk_samples = int(sr * chunk_sec)
    window_samples = int(sr * boundary_window_ms / 1000)

    boundaries = []
    chunk_num = 0

    for pos in range(chunk_samples, len(audio), chunk_samples):
        # å¢ƒç•Œå‰å¾Œã®ã‚¨ãƒãƒ«ã‚®ãƒ¼
        before_start = max(0, pos - window_samples)
        after_end = min(len(audio), pos + window_samples)

        energy_before = np.sqrt(np.mean(audio[before_start:pos] ** 2))
        energy_after = np.sqrt(np.mean(audio[pos:after_end] ** 2))

        # ä½ç›¸é€£ç¶šæ€§ãƒã‚§ãƒƒã‚¯ï¼ˆç°¡æ˜“ç‰ˆï¼‰
        if pos > 10 and pos + 10 < len(audio):
            # å¢ƒç•Œå‰å¾Œ10ã‚µãƒ³ãƒ—ãƒ«ã®ç›¸é–¢
            correlation = np.corrcoef(
                audio[pos-10:pos],
                audio[pos:pos+10]
            )[0, 1]
        else:
            correlation = 1.0

        boundaries.append({
            'chunk': chunk_num,
            'position': pos / sr,
            'energy_before': energy_before,
            'energy_after': energy_after,
            'energy_ratio': energy_after / (energy_before + 1e-10),
            'correlation': correlation,
        })

        chunk_num += 1

    return boundaries


def main():
    print("=" * 80)
    print("éŸ³å£°é€”åˆ‡ã‚Œè¨ºæ–­ã‚¹ã‚¯ãƒªãƒ—ãƒˆ")
    print("=" * 80)

    # è¨­å®šèª­ã¿è¾¼ã¿
    config = RCWXConfig.load()
    model_path = config.last_model_path

    if not model_path or not Path(model_path).exists():
        print("âŒ ã‚¨ãƒ©ãƒ¼: ãƒ¢ãƒ‡ãƒ«ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")
        print("   rcwx ã‚’èµ·å‹•ã—ã¦ãƒ¢ãƒ‡ãƒ«ã‚’é¸æŠã—ã¦ãã ã•ã„")
        return

    print(f"ğŸ“ ãƒ¢ãƒ‡ãƒ«: {Path(model_path).name}")

    # ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³åˆæœŸåŒ–
    print("\nâ³ ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰ä¸­...")
    pipeline = RVCPipeline(model_path, device="auto", use_compile=False)
    pipeline.load()

    # RealtimeVoiceChangeråˆæœŸåŒ–
    rt_config = RealtimeConfig(
        input_device=None,  # ãƒ€ãƒŸãƒ¼å…¥åŠ›
        output_device=None,  # ãƒ€ãƒŸãƒ¼å‡ºåŠ›
        mic_sample_rate=48000,
        input_sample_rate=16000,
        output_sample_rate=48000,
        chunk_sec=0.35,
        pitch_shift=0,
        use_f0=True,
        f0_method="rmvpe",
        use_feature_cache=True,
        use_sola=True,
        voice_gate_mode="off",
        denoise_enabled=False,
    )

    voice_changer = RealtimeVoiceChanger(pipeline, rt_config)

    # ãƒ†ã‚¹ãƒˆä¿¡å·ç”Ÿæˆ
    duration = 5.0
    test_signal = generate_test_signal(duration, sr=48000)

    print(f"\nğŸµ ãƒ†ã‚¹ãƒˆä¿¡å·ç”Ÿæˆ:")
    print(f"   æ™‚é–“: {duration}ç§’")
    print(f"   ã‚µãƒ³ãƒ—ãƒ«ãƒ¬ãƒ¼ãƒˆ: 48000Hz")
    print(f"   æ³¢å½¢: 220Hzæ­£å¼¦æ³¢ + 5HzæŒ¯å¹…å¤‰èª¿")

    # æ‰‹å‹•ã§ãƒãƒ£ãƒ³ã‚¯å‡¦ç†ã‚’ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆ
    print(f"\nâš™ï¸  å‡¦ç†ä¸­ (ãƒãƒ£ãƒ³ã‚¯ã‚µã‚¤ã‚º: {rt_config.chunk_sec}ç§’)...")

    mic_chunk_samples = int(48000 * rt_config.chunk_sec)
    output_chunks = []

    # ChunkBufferã®ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³
    from rcwx.audio.buffer import ChunkBuffer
    from rcwx.audio.resample import resample

    chunk_buffer = ChunkBuffer(
        mic_chunk_samples,
        crossfade_samples=0,
        context_samples=int(48000 * 0.05),
        lookahead_samples=0,
    )

    # å…¥åŠ›ã‚’åˆ†å‰²ã—ã¦å‡¦ç†
    pos = 0
    chunk_count = 0

    voice_changer.pipeline.clear_cache()

    while pos < len(test_signal):
        # ãƒãƒ£ãƒ³ã‚¯ã‚µã‚¤ã‚ºåˆ†å–å¾—
        end = min(pos + mic_chunk_samples, len(test_signal))
        chunk = test_signal[pos:end]

        if len(chunk) < mic_chunk_samples:
            # æœ€å¾Œã®ãƒãƒ£ãƒ³ã‚¯ã¯ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°
            chunk = np.pad(chunk, (0, mic_chunk_samples - len(chunk)), mode='constant')

        # ChunkBufferã«è¿½åŠ 
        chunk_buffer.add_input(chunk)

        # ãƒãƒ£ãƒ³ã‚¯å–å¾—å¯èƒ½ãªã‚‰å‡¦ç†
        if chunk_buffer.has_chunk():
            buffered_chunk = chunk_buffer.get_chunk()

            # ãƒªã‚µãƒ³ãƒ—ãƒ«
            chunk_16k = resample(buffered_chunk, 48000, 16000)

            # RVCæ¨è«–
            output = voice_changer.pipeline.infer(
                chunk_16k,
                input_sr=16000,
                pitch_shift=0,
                f0_method="rmvpe",
                use_feature_cache=True,
                voice_gate_mode="off",
            )

            # ãƒªã‚µãƒ³ãƒ—ãƒ«
            output_48k = resample(output, voice_changer.pipeline.sample_rate, 48000)

            output_chunks.append(output_48k)
            chunk_count += 1

        pos = end

    # å˜ç´”çµåˆï¼ˆã‚¯ãƒ­ã‚¹ãƒ•ã‚§ãƒ¼ãƒ‰ãªã—ï¼‰
    output_simple = np.concatenate(output_chunks)

    print(f"   å‡¦ç†å®Œäº†: {chunk_count}ãƒãƒ£ãƒ³ã‚¯")
    print(f"   å…¥åŠ›é•·: {len(test_signal)} samples ({len(test_signal)/48000:.2f}s)")
    print(f"   å‡ºåŠ›é•·: {len(output_simple)} samples ({len(output_simple)/48000:.2f}s)")

    # åˆ†æ
    print(f"\nğŸ“Š åˆ†æçµæœ:")
    print("-" * 80)

    # 1. ã‚®ãƒ£ãƒƒãƒ—æ¤œå‡º
    gaps = detect_gaps(output_simple, 48000, threshold_db=-40)
    print(f"\nğŸ” æ¤œå‡ºã•ã‚ŒãŸé€”åˆ‡ã‚Œ (ã‚¨ãƒãƒ«ã‚®ãƒ¼ < -40dB):")
    if gaps:
        print(f"   âš ï¸  {len(gaps)}ç®‡æ‰€ã§é€”åˆ‡ã‚Œã‚’æ¤œå‡º!")
        for i, gap in enumerate(gaps[:10]):  # æœ€åˆã®10ä»¶
            print(f"   #{i+1}: {gap['start']:.3f}s - {gap['end']:.3f}s "
                  f"(ç¶™ç¶šæ™‚é–“: {gap['duration']*1000:.1f}ms, "
                  f"æœ€å°ã‚¨ãƒãƒ«ã‚®ãƒ¼: {gap['min_energy']:.1f}dB)")
        if len(gaps) > 10:
            print(f"   ... ä»– {len(gaps)-10}ç®‡æ‰€")
    else:
        print(f"   âœ… é€”åˆ‡ã‚Œãªã—")

    # 2. ãƒãƒ£ãƒ³ã‚¯å¢ƒç•Œåˆ†æ
    boundaries = analyze_chunk_boundaries(output_simple, 48000, rt_config.chunk_sec)
    print(f"\nğŸ” ãƒãƒ£ãƒ³ã‚¯å¢ƒç•Œåˆ†æ ({len(boundaries)}ç®‡æ‰€):")

    energy_drops = [b for b in boundaries if b['energy_ratio'] < 0.5]
    if energy_drops:
        print(f"   âš ï¸  {len(energy_drops)}ç®‡æ‰€ã§ã‚¨ãƒãƒ«ã‚®ãƒ¼ä½ä¸‹ (ratio < 0.5)")
        for b in energy_drops[:5]:
            print(f"   ãƒãƒ£ãƒ³ã‚¯{b['chunk']}: {b['position']:.3f}s, "
                  f"ratio={b['energy_ratio']:.3f}, corr={b['correlation']:.3f}")
    else:
        print(f"   âœ… å¤§ããªã‚¨ãƒãƒ«ã‚®ãƒ¼ä½ä¸‹ãªã—")

    # 3. å¯è¦–åŒ–
    print(f"\nğŸ“ˆ æ³¢å½¢ã¨ã‚¨ãƒãƒ«ã‚®ãƒ¼ã‚’å¯è¦–åŒ–ä¸­...")

    fig, axes = plt.subplots(3, 1, figsize=(14, 10))

    # å…¥åŠ›æ³¢å½¢
    ax1 = axes[0]
    time_in = np.arange(len(test_signal)) / 48000
    ax1.plot(time_in, test_signal, linewidth=0.5, alpha=0.7)
    ax1.set_title("Input Signal (220Hz + 5Hz modulation)")
    ax1.set_xlabel("Time (s)")
    ax1.set_ylabel("Amplitude")
    ax1.grid(True, alpha=0.3)

    # å‡ºåŠ›æ³¢å½¢
    ax2 = axes[1]
    time_out = np.arange(len(output_simple)) / 48000
    ax2.plot(time_out, output_simple, linewidth=0.5, alpha=0.7, color='orange')

    # ãƒãƒ£ãƒ³ã‚¯å¢ƒç•Œã‚’ç¸¦ç·šã§è¡¨ç¤º
    for b in boundaries:
        ax2.axvline(b['position'], color='red', alpha=0.3, linestyle='--', linewidth=1)

    # ã‚®ãƒ£ãƒƒãƒ—é ˜åŸŸã‚’ãƒã‚¤ãƒ©ã‚¤ãƒˆ
    for gap in gaps:
        ax2.axvspan(gap['start'], gap['end'], alpha=0.2, color='red')

    ax2.set_title("Output Signal (with chunk boundaries and gaps)")
    ax2.set_xlabel("Time (s)")
    ax2.set_ylabel("Amplitude")
    ax2.grid(True, alpha=0.3)
    ax2.legend(['Output', 'Chunk boundary', 'Gap'])

    # çŸ­æ™‚é–“ã‚¨ãƒãƒ«ã‚®ãƒ¼
    ax3 = axes[2]
    window = int(48000 * 0.01)
    energies_out = []
    positions_out = []
    for i in range(0, len(output_simple) - window, window // 2):
        energy = np.sqrt(np.mean(output_simple[i:i + window] ** 2))
        energies_out.append(energy)
        positions_out.append(i / 48000)

    ax3.plot(positions_out, energies_out, linewidth=1, color='green')

    # ãƒãƒ£ãƒ³ã‚¯å¢ƒç•Œã‚’ç¸¦ç·šã§è¡¨ç¤º
    for b in boundaries:
        ax3.axvline(b['position'], color='red', alpha=0.3, linestyle='--', linewidth=1)

    ax3.set_title("Short-time Energy (10ms window)")
    ax3.set_xlabel("Time (s)")
    ax3.set_ylabel("RMS Energy")
    ax3.grid(True, alpha=0.3)
    ax3.set_yscale('log')

    plt.tight_layout()

    # ä¿å­˜
    output_dir = Path(__file__).parent / "diagnostic_output"
    output_dir.mkdir(exist_ok=True)

    plot_path = output_dir / "chunk_gap_diagnosis.png"
    wav_path = output_dir / "output_with_gaps.wav"

    plt.savefig(plot_path, dpi=150)
    print(f"   ğŸ“Š ã‚°ãƒ©ãƒ•ä¿å­˜: {plot_path}")

    wavfile.write(wav_path, 48000, output_simple)
    print(f"   ğŸµ éŸ³å£°ä¿å­˜: {wav_path}")

    plt.show()

    # ã‚µãƒãƒªãƒ¼
    print(f"\n" + "=" * 80)
    print("è¨ºæ–­ã‚µãƒãƒªãƒ¼:")
    print("=" * 80)
    print(f"é€”åˆ‡ã‚Œæ¤œå‡º: {len(gaps)}ç®‡æ‰€")
    print(f"ã‚¨ãƒãƒ«ã‚®ãƒ¼ä½ä¸‹: {len(energy_drops)}ç®‡æ‰€ (ãƒãƒ£ãƒ³ã‚¯å¢ƒç•Œ)")

    if gaps or energy_drops:
        print(f"\nâš ï¸  å•é¡ŒãŒæ¤œå‡ºã•ã‚Œã¾ã—ãŸã€‚ä»¥ä¸‹ã‚’ç¢ºèªã—ã¦ãã ã•ã„:")
        print(f"   1. SOLA (use_sola=True) ãŒæœ‰åŠ¹ã‹")
        print(f"   2. Feature Cache (use_feature_cache=True) ãŒæœ‰åŠ¹ã‹")
        print(f"   3. Voice Gate ãŒ 'off' ã¾ãŸã¯ 'expand' ã‹")
        print(f"   4. ãƒãƒ£ãƒ³ã‚¯ã‚µã‚¤ã‚ºãŒé©åˆ‡ã‹ (ç¾åœ¨: {rt_config.chunk_sec}s)")
    else:
        print(f"\nâœ… å¤§ããªå•é¡Œã¯æ¤œå‡ºã•ã‚Œã¾ã›ã‚“ã§ã—ãŸ")

    print(f"\nè©³ç´°ã¯ä»¥ä¸‹ã‚’ç¢ºèª:")
    print(f"   ã‚°ãƒ©ãƒ•: {plot_path}")
    print(f"   éŸ³å£°: {wav_path}")


if __name__ == "__main__":
    main()
